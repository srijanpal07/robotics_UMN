---
title: "Homework 2 - Q8"
author: "Srijan K. Pal"
date: "2023-02-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

14.(a)
```{r}
set.seed(1)
x1=runif(100)
x2=0.5*x1+rnorm(100)/10
y=2+2*x1+0.3*x2+rnorm(100)
```

Linear Model: y = 2 + (2)x1 + (0.3)x2 + e , where e is error term

Regression Coefficients: B0 = 2, B1 = 2, B2 = 0.3

14.(b)
```{r}
cor(x1,x2)
plot(x1,x2)
```

14.(c)
```{r}
lm.fit_1 = lm(y~x1+x2)
summary(lm.fit_1)
```
Predicted coefficients: B0 = 2.1305, B1 = 1.4396, B2 = 1.0097

The standard errors for all the regression coefficients are high. 
The predicted B0 value is very close to the actual value, whereas the predicted B1 and B2 are off by about 0.7 in both the case.

We can reject the null hypothesis for B1 because the p-values is comparatively low. But, the null hypothesis cannot be rejected for B2 as the p-value is considerably high in this case. 


14.(d)
```{r}
lm.fit_2 = lm(y~x1)
summary(lm.fit_2)
```
We can neglect the null hypothesis in this case, as for the regression coefficient, the p-value is very close to zero.


14.(e)
```{r}
lm.fit_3 = lm(y~x2)
summary(lm.fit_3)
```
We can neglect the null hypothesis in this case, as for the regression coefficient, the p-value is very close to zero.


14.(f) No the results does not contradict each other. The results from (c) simply means the x1 has much more effect on the prediction compared to x2 when they are both considered in the regression. But when they are performed in regression separately, we come to know the relationship between the response and each of the predictor more clearly. 

14.(g)
```{r}
x1 = c(x1,0.1)
x2 = c(x2,0.8)
y = c(y,6)
```
```{r}
lm.fit_4 = lm(y~x1+x2)
summary(lm.fit_4)
```
The standard errors for both B1 and B2 are high. 
We can reject the null hypothesis for B2 because the p-values is comparatively low. But, the null hypothesis cannot be rejected for B1 as the p-value is considerably high in this case.


```{r}
lm.fit_5 = lm(y~x1)
summary(lm.fit_5)
```
We can neglect the null hypothesis in this case, as for the regression coefficient x1, the p-value is very close to zero.


```{r}
lm.fit_6 = lm(y~x2)
summary(lm.fit_6)
```
We can neglect the null hypothesis in this case, as for the regression coefficient x2, the p-value is very close to zero.


```{r}
par(mfrow=c(2,2))
plot(lm.fit_4)
```

Point 101 is a high leverage point.

```{r}
par(mfrow=c(2,2))
plot(lm.fit_5)
```

```{r}
par(mfrow=c(2,2))
plot(lm.fit_6)
```

Point 101 is a high leverage point.

```{r}
plot(predict(lm.fit_4), rstudent(lm.fit_4))
```

From the studentized residual plots, it is clearly visible that there is less possibility of outliers in the the data as none of the data points have values greater than 3.

Point 101 is a high leverage point from the previous plot but is not an outlier


```{r}
plot(predict(lm.fit_5), rstudent(lm.fit_5))
```

From the studentized residual plots, it is clearly visible that there is less possibility of outliers in the the data as only one of the data points have values greater than 3.

Therfor point 101 is an outlier

```{r}
plot(predict(lm.fit_6), rstudent(lm.fit_6))
```

From the studentized residual plots, it is clearly visible that there is less possibility of outliers in the the data as none of the data points have values greater than 3.

Point 101 is not an outlier